{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c3967c-121c-4986-a7cc-8fd2e5d6bfc5",
   "metadata": {},
   "source": [
    "# Extract particle VDFs from FLEKS output and select desired particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb2d9d",
   "metadata": {},
   "source": [
    "This note book describes how to read FLEKS particle data output. First, you would need to install SWMF and enable FLEKS in it. \n",
    "During the time this jupyter notebook was created, the FLEKS provided some Python wrapper files on yt package to read the data output, in this case, you need to put FLEKS python helper library into the PYTHONPATH.\n",
    "Later on, this wrapper files have been moved to a python pacakge called flekspy, detailed instructions can be found here: https://github.com/henry2004y/flekspy\n",
    "The usages are not changed, the only difference in terms of what this file covers is the name of the pacakge changed from fleks to flekspy. flekspy may support more functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4f647",
   "metadata": {},
   "source": [
    "The following cell covers the importation of the pacakges. two util functions are also defined here.\n",
    "1. ```rotation_matrix_from_vectors``` calculates the unit rotation matrix that rotates vec1 to vec2. This is function is needed because we need to rotate the velocity vector that aligns with the local magnetic field coordinates.\n",
    "2. ```calc_vb_vectors``` constructs a local cartesian coordinate defined by the local bulk velocity and local magnetic field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2403e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fleks, yt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import ticker\n",
    "\n",
    "from matplotlib import  rcParams\n",
    "\n",
    "rcParams['text.usetex'] = False\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['axes.linewidth'] = 2\n",
    "rcParams['xtick.major.width'] = 1\n",
    "rcParams['xtick.major.size'] = 4\n",
    "rcParams['xtick.minor.width'] = 1\n",
    "rcParams['xtick.minor.size'] = 2\n",
    "rcParams['ytick.major.width'] = 1\n",
    "rcParams['ytick.major.size'] = 4\n",
    "rcParams['ytick.minor.width'] = 1\n",
    "rcParams['ytick.minor.size'] = 2\n",
    "\n",
    "def rotation_matrix_from_vectors(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Find the rotation matrix that aligns vec1 to vec2\n",
    "    :param vec1: A 3d \"source\" vector\n",
    "    :param vec2: A 3d \"destination\" vector\n",
    "    :return mat: A transform matrix (3x3) which when applied to vec1, aligns it with vec2.\n",
    "    \"\"\"\n",
    "\n",
    "    a = (vec1 / np.linalg.norm(vec1)).reshape(3)\n",
    "    b = (vec2 / np.linalg.norm(vec2)).reshape(3)\n",
    "    v = np.cross(a, b)\n",
    "    c = np.dot(a, b)\n",
    "    s = np.linalg.norm(v)\n",
    "    kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
    "    rotation_matrix = np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2))\n",
    "    return rotation_matrix\n",
    "\n",
    "def calc_vb_vectors(bvec, vvec):\n",
    "    unit_b = (bvec / np.linalg.norm(bvec)).reshape(3)\n",
    "    unit_v = (vvec / np.linalg.norm(vvec)).reshape(3)\n",
    "    unit_vb = np.cross(unit_v, unit_b)\n",
    "    unit_perp = np.cross(unit_b, unit_vb)\n",
    "    unit_vb = unit_vb / np.linalg.norm(unit_vb)\n",
    "    unit_perp = unit_perp / np.linalg.norm(unit_perp)\n",
    "    return unit_b, unit_vb, unit_perp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f652a",
   "metadata": {},
   "source": [
    "The following cell is defining paths for the data files. ```smaple_points_dict``` stores the location, magnetic field, and local bulk velocity of a sampling point.\n",
    "Notice that if we only need VDF, magnetic field and local bulk velocity are not needed. The reason we need these two data is we want to subtract the bulk velocity and project the velocity on B and VxB coordinates (this is also what MMS observation does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "file_root = './'\n",
    "data_file_dict = {\n",
    "      '1321': {'e':'{:s}/3d_particle_region0_3_t00025100_n00102600_amrex'.format(file_root),  \n",
    "               'i':'{:s}/na'.format(file_root)},\n",
    "      '1336': {'e':'{:s}/3d_particle_region0_3_t00030600_n00111600_amrex'.format(file_root),\n",
    "               'i':'{:s}/na'.format(file_root)},\n",
    "}\n",
    "\n",
    "sample_points_dict = {\n",
    "\n",
    "      \"A1336\": { # bz max\n",
    "            \"center\":[-9.03117547529323872,-6.84825371941971106,0.66096325359352126],\n",
    "            \"bvec\":[0,-9.48951247597609182, 5.32121735196562717], \n",
    "            \"vvec\":[125.220516405888446,93.2773115522788743,109.492633538466436]},\n",
    "\n",
    "      \"B1336\": { # bz max\n",
    "            \"center\":[-9.76964755139845131,-7.32371192407870986,1.24371880616626251],\n",
    "            \"bvec\":[0,-4.05134576555627035, 22.0761419531317387], \n",
    "            \"vvec\":[199.169579060765443,72.9757710693565684,37.52071508065481]},\n",
    "      \n",
    "      \"C1336\": { # bz max\n",
    "            \"center\":[-11.4312097226351757,-7.94020753612441954,1.21509465825193352],\n",
    "            \"bvec\":[0,1.25788633747233325,5.29431906060982449], \n",
    "            \"vvec\":[428.146766109107034,159.899385314072674,19.4124550787820915]},\n",
    "      \n",
    "      \"D1336\": { # bz max\n",
    "            \"center\":[-17.2159076521259351,-8.79224491860180279,1.55253240743197751],\n",
    "            \"bvec\":[0,0.702954102301058814,6.52342068511395912], \n",
    "            \"vvec\":[592.927637537339933,209.063232654131639,-24.5355648076267698]},\n",
    "      \n",
    "      \"E1336\": { # bz max\n",
    "            \"center\":[-21.3390434103800004,-7.48264787712713009,2.12330857508734328],\n",
    "            \"bvec\":[0,-0.114815775454145189,2.22874239718768008], \n",
    "            \"vvec\":[549.030277551509926,245.802692208670237,19.6825811397322603]},\n",
    "      \n",
    "      \"F1336\": { # bz max\n",
    "            \"center\":[-23.2467629403184333,-5.94394102064707397,2.10841056981084085],\n",
    "            \"bvec\":[0,0.438817533054258357,3.28264870758951766], \n",
    "            \"vvec\":[427.238250040752348,131.772730249905493,5.84478385225656893]},\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed6488",
   "metadata": {},
   "source": [
    "The next cell is the code to load the dataset.\n",
    "```ds = fleks.load(data_file)``` ds is is a yt dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ade3d-1cfb-47f8-b2eb-d32791e89c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptype = 'e'\n",
    "point = \"B1321\"\n",
    "\n",
    "def _unit_one(field, data):        \n",
    "    res = np.zeros(data[('particles', 'p_w')].shape)\n",
    "    res[:] = 1\n",
    "    return res\n",
    "\n",
    "data_file = data_file_dict[point[-4:]][ptype]\n",
    "print(data_file)\n",
    "ds = fleks.load(data_file)\n",
    "# Add a user defined field. See yt document for more information about derived field.\n",
    "ds.add_field(ds.pvar(\"unit_one\"), function=_unit_one, sampling_type='particle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3196cb6",
   "metadata": {},
   "source": [
    "The following cell is the core code to plot a VDF from a certain region. \n",
    "\n",
    "To be able to directly compare with MMS observations, particle velocities are usually projected to directions parallel/perpendicular to the local magnetic field with the local bulk velocity substracted.\n",
    "\n",
    "Here notice that the AMRex data output is manipulated using yt package. Hence we need to create an array that stores the bulk velocity:\n",
    "\n",
    "```python \n",
    "u_bulk = ds.arr(sample_points_dict[point][\"vvec\"], \"code_velocity\")\n",
    "```\n",
    "\n",
    "and the substraction is done by defining an internal function first and calling ```add_field```:\n",
    "\n",
    "```python\n",
    "def _vel_ux_vframe(field, data):\n",
    "    res = data['particles', 'p_ux'] - u_bulk[0]\n",
    "    return res\n",
    "```\n",
    "\n",
    "Similar process is applied on velocity projection.\n",
    "\n",
    "The VDF is essentially a phase space density plot, this is done by \n",
    "\n",
    "```python\n",
    "profile = yt.create_profile(data_source=source_region, bin_fields=[x_field,y_field], \n",
    "                                fields=z_field, n_bins=[64, 64], weight_field=None, logs=logs)\n",
    "plot = yt.PhasePlot.from_profile(profile)\n",
    "```\n",
    "here ``` plot ``` variable is used to access the plot data, such as x, y, z for customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ae4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=int(max(len(sample_points_dict), 2) / 2), figsize=(14, 10))\n",
    "fig.tight_layout()\n",
    "point_list = reversed(sample_points_dict.keys())\n",
    "\n",
    "for iplot, point in enumerate(point_list):\n",
    "    plot = None\n",
    "    \n",
    "    # calculate velocity in bulk velocity frame\n",
    "    u_bulk = ds.arr(sample_points_dict[point][\"vvec\"], \"code_velocity\")\n",
    "\n",
    "    def _vel_ux_vframe(field, data):\n",
    "        res = data['particles', 'p_ux'] - u_bulk[0]\n",
    "        return res\n",
    "    def _vel_uy_vframe(field, data):\n",
    "        res = data['particles', 'p_uy'] - u_bulk[1]\n",
    "        return res\n",
    "    def _vel_uz_vframe(field, data):\n",
    "        res = data['particles', 'p_uz'] - u_bulk[2]\n",
    "        return res\n",
    "    ds.add_field(ds.pvar(\"ux_bulk_frame\"), units=\"code_velocity\", function=_vel_ux_vframe, sampling_type='particle', force_override=True)\n",
    "    ds.add_field(ds.pvar(\"uy_bulk_frame\"), units=\"code_velocity\", function=_vel_uy_vframe, sampling_type='particle', force_override=True)\n",
    "    ds.add_field(ds.pvar(\"uz_bulk_frame\"), units=\"code_velocity\", function=_vel_uz_vframe, sampling_type='particle', force_override=True)\n",
    "\n",
    "    # project velocity to B, VxB, and the third direction\n",
    "    unit_b, unit_vb, unit_bvb = calc_vb_vectors(sample_points_dict[point][\"bvec\"], \n",
    "                                        sample_points_dict[point][\"vvec\"])\n",
    "    def _vel_b(field, data):\n",
    "        res = unit_b[0]*data[('particles', 'ux_bulk_frame')] + unit_b[1]*data[('particles', 'uy_bulk_frame')] + unit_b[2]*data[('particles', 'uz_bulk_frame')]        \n",
    "        return res\n",
    "    def _vel_vb(field, data):\n",
    "        res = unit_vb[0]*data[('particles', 'ux_bulk_frame')] + unit_vb[1]*data[('particles', 'uy_bulk_frame')] + unit_vb[2]*data[('particles', 'uz_bulk_frame')]        \n",
    "        return res\n",
    "    def _vel_bvb(field, data):\n",
    "        res = unit_bvb[0]*data[('particles', 'ux_bulk_frame')] + unit_bvb[1]*data[('particles', 'uy_bulk_frame')] + unit_bvb[2]*data[('particles', 'uz_bulk_frame')]        \n",
    "        return res\n",
    "    ds.add_field(ds.pvar(\"vel_b\"), units=\"code_velocity\", function=_vel_b, sampling_type='particle', force_override=True)\n",
    "    ds.add_field(ds.pvar(\"vel_vb\"), units=\"code_velocity\", function=_vel_vb, sampling_type='particle', force_override=True)\n",
    "    ds.add_field(ds.pvar(\"vel_bvb\"), units=\"code_velocity\", function=_vel_bvb, sampling_type='particle', force_override=True)\n",
    "\n",
    "    center = np.array(sample_points_dict[point][\"center\"])\n",
    "    box_size = 0.8\n",
    "    left_edge = center - 0.5 * box_size\n",
    "    right_edge = center + 0.5 * box_size\n",
    "\n",
    "    var_x_name = \"vel_vb\"\n",
    "    var_y_name = \"vel_b\"\n",
    "    source_region = ds.box(left_edge, right_edge)\n",
    "\n",
    "    x_field = (\"particles\", var_x_name)\n",
    "    y_field = (\"particles\", var_y_name)\n",
    "    z_field = ('particles','unit_one')\n",
    "\n",
    "    logs = {x_field: False, y_field: False}\n",
    "    profile = yt.create_profile(data_source=source_region, bin_fields=[x_field,y_field], \n",
    "                                fields=z_field, n_bins=[64, 64], weight_field=None, logs=logs)\n",
    "    plot = yt.PhasePlot.from_profile(profile)\n",
    "    plot.set_figure_size(6)\n",
    "    plot.set_cmap(plot.fields[0], \"jet\")\n",
    "    plot.set_colorbar_label(plot.fields[0], \" \")\n",
    "\n",
    "    for var_name in plot.profile.field_data: \n",
    "        val = np.array(plot.profile.field_data[var_name])\n",
    "    \n",
    "    x = np.array(plot.profile.x)\n",
    "    y = np.array(plot.profile.y)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "    cur_ax = axes.flatten()[iplot]\n",
    "    cur_ax.contourf(\n",
    "                        xx, yy, np.log(np.array(np.transpose(val))),\n",
    "                        cmap='jet',\n",
    "                        levels=60,\n",
    "                        #vmin = 44.9, vmax = 55.1\n",
    "                    )\n",
    "    cur_ax.set_aspect('equal')\n",
    "    cur_ax.set_xlim([-15000,15000])\n",
    "    cur_ax.set_ylim([-15000,15000])\n",
    "    cur_ax.set_xlabel(r\"$V_{V \\times B}\\ [km/s]$\")\n",
    "    if iplot == 0:\n",
    "        cur_ax.set_ylabel(r\"$V_B\\ [km/s]$\")\n",
    "    cur_ax.set_title(point[0])\n",
    "    cur_ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502df875",
   "metadata": {},
   "source": [
    "OK, now we have the VDF of particles in a specific region, what if we want to get the detailed data of the particles inside this region? There is a convenient way to directly access particle data, here is an example of accessing the ```x``` coordinate of particles in a region:\n",
    "```python\n",
    "np.array(yt_region['particles', 'particle_position_x'])\n",
    "```\n",
    "notice that, we need to firstly define a yt region. The following function ``` extract_particle_in_region``` can return a pandas dataframe that contains the particle data inside this region.\n",
    "\n",
    "DO NOT try to call this function for entire PIC domain, it will never finish and use up your system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb378b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_particle_in_region(yt_region):\n",
    "    df_particle = pd.DataFrame.from_dict({\n",
    "        'cpu': np.array(yt_region['particles', 'particle_cpu'], dtype=np.intc),\n",
    "        'supid': np.array(yt_region['particles', 'particle_supid'], dtype=np.longlong),\n",
    "        'id': np.array(yt_region['particles', 'particle_id'], dtype=np.longlong),\n",
    "        'x': np.array(yt_region['particles', 'particle_position_x']),\n",
    "        'y': np.array(yt_region['particles', 'particle_position_y']),\n",
    "        'z': np.array(yt_region['particles', 'particle_position_z']),\n",
    "        'ux': np.array(yt_region['particles', 'particle_velocity_x']),\n",
    "        'uy': np.array(yt_region['particles', 'particle_velocity_y']),\n",
    "        'uz': np.array(yt_region['particles', 'particle_velocity_z']),\n",
    "        'weight': np.array(yt_region['particles', 'particle_weight']),\n",
    "    })\n",
    "    df_particle['energy'] = (\n",
    "                                0.5 *\n",
    "                                #df_particle['weight'] * \n",
    "                                (df_particle['ux']**2 + df_particle['uy']**2 + \n",
    "                                 df_particle['uz']**2) *\n",
    "                                1.66054e-27 * # amu to kg\n",
    "                                1e3 ** 2 # km/s to m/s squared\n",
    "                                * 6.242e+18 # joule to eV\n",
    "                            )\n",
    "    return df_particle\n",
    "\n",
    "point = \"B1336\"\n",
    "center = np.array(sample_points_dict[point][\"center\"])\n",
    "box_size = 0.8\n",
    "left_edge = center - 0.5 * box_size\n",
    "right_edge = center + 0.5 * box_size\n",
    "yt_region = ds.box(left_edge, right_edge)\n",
    "df_particle = extract_particle_in_region(yt_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6dab19",
   "metadata": {},
   "source": [
    "Once you have the particle dataframe, you can select particles inside this region based on your need. Notice that to trace particle in previous and subsequent SWMF restart trees, you need to save cpu, supid and id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66029b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_velocity(df, b_vec, u_bulk):\n",
    "  unit_b, unit_vb, unit_bvb = calc_vb_vectors(b_vec, u_bulk)\n",
    "  assert(np.abs(np.linalg.norm(unit_b) - 1.0) < 1e-6)\n",
    "  assert(np.abs(np.linalg.norm(unit_vb) - 1.0) < 1e-6)\n",
    "  assert(np.abs(np.linalg.norm(unit_bvb) - 1.0) < 1e-6)\n",
    "  df[\"ux_bulk_frame\"] = df[\"ux\"] - u_bulk[0]\n",
    "  df[\"uy_bulk_frame\"] = df[\"uy\"] - u_bulk[1]\n",
    "  df[\"uz_bulk_frame\"] = df[\"uz\"] - u_bulk[2]\n",
    "  df[\"vel_b\"] = unit_b[0]*df[\"ux_bulk_frame\"] + unit_b[1]*df[\"uy_bulk_frame\"] + unit_b[2]*df[\"uz_bulk_frame\"]\n",
    "  df[\"vel_vb\"] = unit_vb[0]*df[\"ux_bulk_frame\"] + unit_vb[1]*df[\"uy_bulk_frame\"] + unit_vb[2]*df[\"uz_bulk_frame\"]\n",
    "  df[\"vel_bvb\"] = unit_bvb[0]*df[\"ux_bulk_frame\"] + unit_bvb[1]*df[\"uy_bulk_frame\"] + unit_bvb[2]*df[\"uz_bulk_frame\"]\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6372323",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_vec = sample_points_dict[point][\"bvec\"]\n",
    "u_bulk = sample_points_dict[point][\"vvec\"]\n",
    "df_new = project_velocity(df_particle, b_vec, u_bulk)\n",
    "df_new['vel_amp'] = np.sqrt(df_new['vel_vb']*df_new['vel_vb'] + df_new['vel_b']*df_new['vel_b'])\n",
    "\n",
    "# check low energy particles\n",
    "a, b = 1500, 2000\n",
    "df_new['inside_ellipse'] = (df_new['vel_vb'] / a) ** 2 + (df_new['vel_b'] / b) **2 \n",
    "\n",
    "r = 5000\n",
    "df_new['inside_circle'] = (df_new['vel_vb'] / r) ** 2 + (df_new['vel_b'] / r) **2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a864446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_new.loc[(df_new['inside_ellipse'] <= 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60635a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.to_csv('output.csv', columns=['cpu', 'supid', 'id'],index=False, \n",
    "                    header=None, sep=\" \")\n",
    "with open('output.csv', 'r') as original:\n",
    "    data = original.read()\n",
    "with open('select_particle_low_new.in', 'w') as modified:\n",
    "    modified.write(str(len(df_selected))+\"\\n\" + data)\n",
    "\n",
    "df_selected.to_csv('df_1321_low_new.csv',index=False, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = [1334, 1335, 1337]\n",
    "df_dict = {}\n",
    "\n",
    "for time in time_list:\n",
    "  df= pd.read_csv(\"select_particle_{:d}.out\".format(time), sep=\"\\s+\")\n",
    "  for key in ['x', 'y', 'z']:\n",
    "    df[key] = df[key] / 6378000\n",
    "  df_dict[time] = df\n",
    "\n",
    "df_dict[1336] = df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10865e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(df_dict), ncols=1, figsize=(6, 10))\n",
    "\n",
    "for iax, time in enumerate(sorted(df_dict.keys())):\n",
    "  df = df_dict[time]\n",
    "  axes[iax].scatter(df['x'], df['y'], s=0.5)\n",
    "  axes[iax].set_xlim(-40, 0)\n",
    "  axes[iax].set_ylim(-10, 10)\n",
    "  axes[iax].annotate('{:d}'.format(time), xy =(-35, 6), fontsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(df_dict), ncols=1, figsize=(6, 10))\n",
    "\n",
    "for iax, time in enumerate(sorted(df_dict.keys())):\n",
    "  df = df_dict[time]\n",
    "  axes[iax].scatter(df['x'], df['z'], s=0.5)\n",
    "  axes[iax].set_xlim(-40, 0)\n",
    "  axes[iax].set_ylim(-10, 10)\n",
    "  axes[iax].annotate('{:d}'.format(time), xy =(-35, 6), fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
